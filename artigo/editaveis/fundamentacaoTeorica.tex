\chapter[Fundamentação teórica]{Fundamentação teórica}

Este capítulo destina-se a apresentar a fundamentação teórica realizada por meio de revisões da literatura. Seu objetivo é oferecer um catálogo de conceitos que servirá como base para o entendimento do atual trabalho.

\section[Processamento de Linguagem	 Natural]{Processamento de Linguagem Natural}

Segundo Christian Aranha (2007), “o Processamento de Linguagem Natural (PLN) é o campo da ciência que abrange um conjunto de métodos formais para analisar textos e gerar frases em um idioma humano.”

No atual projeto, técnicas de PLN estarão fortemente presentes durante a etapa de pré-processamento do texto. O objetivo é extrair o máximo de features que consideramos importante para a classificação adequada. Features são termos ou expressões presentes em um dado texto consideradas determinantes para se classificá-lo.

Dependendo do tipo de classificação que se deseja, diferentes técnicas de processamento são utilizadas. Quando se classifica uma frase utilizando análise de sentimento, ou seja, classificando a frase como positiva ou negativa, técnicas de processamento como a POS (\textit{Part Of Speech}) \textit{tagging}, que tem como objetivo identificar suas classes gramaticais, são utilizadas de modo que se melhore a precisão da classificação da frase. Já quando uma frase é classificada como discurso de ódio ou não, técnicas como a \textit{Bag of Words} (saco de palavras) e a \textit{Word Generalisation} (Generalização de palavras) são preferíveis.

As técnicas citadas são utilizadas na etapa de extração de atributos, comumente referida como \textit{Feature Extraction}. É a partir dela que se define quais aspectos do texto são importantes, ou seja, o que o algoritmo de classificação usará. Há diversas formas de se abordar a extração de características de um texto, sendo as principais: \textit{Bag-of-Words}, \textit{n-grams}, \textit{Word2Vec} e TF-IDF.

\subsection{Bag-of-Words}

O BoW (\textit{Bag-of-Words}) é um modelo de extração de características de texto bastante flexível, ele descreve o número de ocorrências de palavras de uma frase. Para isso, cada frase é representada por um vetor com n elementos, onde n é o número de palavras do vocabulário. Cada posição representa uma palavra e o elemento é o número de ocorrências daquela palavra na frase. Observe os exemplos a seguir:

\begin{samepage}
(1) “o dia está radiante”

\nopagebreak

(2) “que dia é hoje?”
\end{samepage}

Imagine um vetor para cada uma dessas frases. No BoW, eles seriam semelhantes a estes:

\begin{center}
\begin{table}[htbp]
\centering
\begin{tabular}{c|ccccccc}
 & \textbf{o} & \textbf{dia} & \textbf{está} & \textbf{radiante} & \textbf{que} & \textbf{é} & \textbf{hoje} \\ \hline
\textbf{Vetor 1} & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\
\textbf{Vetor 2} & 0 & 1 & 0 & 0 & 1 & 1 & 1 
\end{tabular}
\caption{Exemplo de \textit{Bag-of-Words}}
\label{bow}
\end{table}
\end{center}

Como é possível observar na Tabela \ref{bow}, com apenas duas pequenas frases, o vocabulário é bastante limitado, apenas sete palavras, porém, o que é mais comum em situações reais são vocabulários absurdamente grandes, o que resulta em vetores igualmente grandes e com quase todos os seus elementos iguais a 0.

Outro fator que vale ser notado é que a ordem das palavras não são levadas em consideração neste modelo, apenas a existência delas.

\subsection{N-grams}

O modelo \textit{n-grams} é bastante similar ao \textit{Bag-of-Words} no sentido de definir um vetor para a frase e realizar uma contagem de ocorrências. A diferença está na \textit{vetorização}, que se dá por um processo diferente. Ao invés de representar palavras do vocabulário nas posições do vetor, o \textit{n-grams} propõe uma representação por \textit{substrings}  com um número \textit{N} de palavras. Desta forma, este modelo se torna mais adequado para capturar nuances do texto (CITAR: Lundborg, 2017 Master Thesis), onde a ordem das palavras é relevante e não apenas a presença delas. Utilizando as frases do exemplo anterior, observe como os vetores ficariam para \textit{N = 2}.

\begin{samepage}
(1) “o dia está radiante”

\nopagebreak

(2) “que dia é hoje?”
\end{samepage}

\begin{center}
\begin{table}[htbp]
\centering
\begin{tabular}{c|cccccc}
 & \textbf{o dia} & \textbf{dia está} & \textbf{está radiante} & \textbf{que dia} & \textbf{dia é} & \textbf{é hoje} \\ \hline
\textbf{Vetor 1} & 1 & 1 & 1 & 0 & 0 & 0 \\
\textbf{Vetor 2} & 0 & 0 & 0 & 1 & 1 & 1
\end{tabular}
\caption{Exemplo de \textit{n-grams}}
\label{ngrams}
\end{table}
\end{center}

\subsection{TF-IDF}

Em algumas situações, apenas avaliar a frequência de termos pode não ser o ideal. A forma com que a língua é estruturada favorece a dominância de uma série de termos dos quais não carregam a em si conteúdo significativo para a realização de classificações (CITAR: Brownlee, Jason).

A fim de reduzir o grau de relevância desses termos que predominam a linguagem naturais, temos o modelo \textit{TF-IDF}, que significa \textit{Term Frequency-Inverse Document Frequency}, onde:

\begin{itemize}
\item \textit{Term Frequency}: Do inglês, significa frequência do termo dentro da frase atual. No caso, termo pode ser uma palavra ou múltiplas, como no modelo N-grams.
\item \textit{Inverse Document Frequency}: Trata-se de um valor inverso à frequência em que o termo é encontrado nas frases do conjunto de dados como um todo.
\end{itemize}

Para calcular o \textit{TF-IDF}, utiliza-se a seguinte fórmula:

\begin{equation}
w_{i,j} = tf_{i,j}\times\log \frac{N}{df_i}
\end{equation}

Observe que, ao aplicar esta fórmula obtém-se um valor que é mais alto quando o termo for frequente no texto analisado, porém, raro em todos os textos, ou seja, as particularidades de cada texto são acentuadas.

\subsection{Limpeza do Texto}

Parte do pré-processamento do texto é a realização de sua limpeza. Para tal, foram utilizadas duas técnicas:

\begin{itemize}
\item \textbf{\textit{Stemização}}: Do inglês, \textit{stemming}, refere-se ao processo de reduzir as palavras à uma forma primitiva, como um radical. Este processo visa garantir que pequenas variações de uma mesma palavra seja interpretada pelo computador como uma palavra só. Dessa forma, além de economizar recursos, pois o computador precisará processar um vetor de menor dimensão, também beneficiará o classificador considerar que palavras com pequenas variações de gênero ou número carregam em si o mesmo sentido. Como em “\textit{historiador}” e “\textit{historiadoras}”, que ficariam apenas “\textit{histori}”, por exemplo.

\item \textbf{Remoção das \textit{stop words}}: As \textit{stop words} (palavras vazias) são palavras que agregam pouco ou nenhum valor semântico que deva ser levado em consideração pelo classificador. Geralmente, são as palavras mais comuns da língua, incluindo artigos, preposições, verbos de ligação, entre outras. Não existe um conjunto bem definido de quais palavras devem ser classificadas como palavras vazias, e, naturalmente, esse conjunto depende do idioma em questão. Para o atual projeto, foi utilizado o conjunto em português do \textbf{NLTK}.
\end{itemize}