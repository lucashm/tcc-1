\chapter[Fundamentação teórica]{Fundamentação teórica}

Este capítulo destina-se a apresentar a fundamentação teórica realizada por meio de revisões da literatura. Seu objetivo é oferecer um catálogo de conceitos que servirá como base para o entendimento do atual trabalho.

\section[Processamento de Linguagem	 Natural]{Processamento de Linguagem Natural}

O Processamento de Linguagem Natural é uma ciência que abranje um conjunto de técnicas e métodos que facilitam a análise e textual por um computador \cite{aranha}. 
No atual projeto, técnicas de PLN são aplicadas para extrair as \textit{features} que, por sua vez, serão utilizadas para a realização das classificações. No contexto do aprendizado de máquina, as \textit{features} são as variáveis intrinsecamente presentes nos dados, sendo elas essenciais na identificação de padrões pelo algoritmo.

As técnicas citadas são utilizadas na etapa de extração de atributos, comumente referida como \textit{Feature Extraction}. É a partir dela que se define quais aspectos do texto servirão de insumo para o classificador. Há diversas formas de se abordar a extração de características de um texto, sendo as principais: \textit{Bag-of-Words}, \textit{n-grams}, \textit{Word2Vec} e TF-IDF.

\subsection{Bag-of-Words}

O BoW (\textit{Bag-of-Words}) é um modelo de extração de características de texto simples e flexível. Ele se baseia no número de ocorrências de palavras de uma frase. Para isso, cada frase é representada por um vetor com \textit{n} elementos, onde \textit{n} é o número de palavras do vocabulário considerado. Cada posição representa uma palavra e o elemento é o número de ocorrências daquela palavra na frase. Observe os exemplos a seguir:

\begin{samepage}
(1) \textit{u} =  “A mochila está leve, então quero a leve”

\nopagebreak

(2) \textit{v} = “Levei um lache leve na mochila”
\end{samepage}

Imagine um vetor para cada uma dessas frases. No BoW, eles seriam semelhantes a estes:

\begin{center}
\begin{table}[htbp]
\centering
\begin{tabular}{ccccccclllll}
\hline
 & \textbf{a} & \textbf{mochila} & \textbf{está} & \textbf{leve} & \textbf{então} & \textbf{quero} & \textbf{que} & \textbf{levei} & \textbf{um} & \textbf{lanche} & \textbf{na} \\ \hline
\textbf{\textit{u}} & 2 & 1 & 1 & 2 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
\textbf{\textit{v}} & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 1
\end{tabular}
\caption{Exemplo de \textit{Bag-of-Words}}
\label{bow}
\end{table}
\end{center}

Como é possível observar na Tabela \ref{bow}, a palavra "leve" foi utilizada duas vezes na frase \textit{v}, sendo, na primeira, um adjetivo e, na segunda, um verbo. São palavras homônimas, que possuem a mesma grafia, mas significados distintos. Este modelo, porém, desconsidera o papel que as palavras desempenham na frase e a sua estrutura.

Com apenas duas pequenas frases, o vocabulário é bastante limitado, apenas 11 palavras, porém, o que é mais comum em situações reais são vocabulários muito maiores, o que pode resultar em vetores igualmente maiores e com quase todos os seus elementos iguais a 0. Na linguagem \textit{Python}, utilizada neste projeto, esses vetores são representados por matrizes esparsas. Observe a seguir como a classe \textit{CountVectorizer}, da biblioteca scikit, trabalha com essas matrizes.

\begin{lstlisting}[language=python, caption=Matriz esparsa da classe csr\_matrix, label={lst:csr_matrix}]

>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [
        'A mochila está leve, então quero a leve primeiro',
        'Levei um lanche leve na mochila',
    ]
>>> vectorizer = CountVectorizer()
>>> x = vectorizer.fit_transform(corpus)
>>> print(vectorizer.get_feature_names())

['então', 'está', 'lanche', 'leve', 'levei', 'mochila', 'na', 'primeiro', 'quero', 'um']

>>> print(x.toarray())

[[1 1 0 2 0 1 0 1 1 0]
 [0 0 1 1 1 1 1 0 0 1]]
 
\end{lstlisting}

Note no Código \ref{lst:csr_matrix}, que o \textit{CountVectorizer} aplica o modelo de \textit{Bag-of-Words} na construção de um objeto do tipo \textit{csr\_matrix}, mas não leva em consideração palavras com apenas um caractere.

\subsection{N-grams}

O modelo \textit{n-grams}, assim como o \textit{Bag-of-Words}, baseia-se em uma contagem de ocorrências. A diferença está na \textit{vetorização}, que se dá por um processo diferente. Ao invés de realizar a contagem por cada palavra, o \textit{n-grams} propõe uma contagem por uma sequência contínua de \textit{n} de palavras. Desta forma, este modelo se torna mais adequado para capturar nuances do texto \cite{Lundborg_2017}, onde a ordem das palavras é relevante e não apenas a presença delas. Utilizar este modelo também favorece na identificação de palavras compostas como "primeiro-ministro", "Rio de Janeiro", "Aprendizado de Máquina".

\begin{samepage}
(1) \textit{u} = “Eu moro no Distrito Federal”

\nopagebreak

(2) \textit{v} = “O Distrito Federal fica no Centro-Oeste”
\end{samepage}

\begin{center}
\begin{table}[htbp]
\centering
\begin{tabular}{cccccccccc}
\hline
 & \textbf{\begin{tabular}[c]{@{}c@{}}eu\\ moro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}moro\\ no\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}no\\ distrito\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}distrito\\ federal\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}o\\ distrito\end{tabular}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}federal\\ fica\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}fica\\ no\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}no\\ centro\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}centro\\ oeste\end{tabular}}} \\ \hline
\textit{\textbf{u}} & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
\textit{\textbf{v}} & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1
\end{tabular}
\caption{Exemplo de representação do modelo \textit{n-grams}}
\label{tab:ngrams}
\end{table}
\end{center}

Observe, na Tabela \ref{tab:ngrams}, como os vetores ficariam para \textit{n} = 2 e note que este modelo é capaz de identificar a presença do \textit{bigrama}\footnote{\textit{n-grams} com \textit{n} = 2} "distrito federal" em ambas as frases. Outra situação que favorece o uso de \textit{n-grams} é quando, por exemplo, há a presença das palavras "distrito" e "federal", mas elas estão separadas e com outro sentido, como na frase \textit{z} abaixo. O modelo BoW não seria capaz de realizar essa distinção e identificaria uma falsa semelhança entre as frases \textit{u} e \textit{z} pela presença de termos que, individualmente são iguais, mas que dentro de seus contextos, possuem outros sentidos.

(3) \textit{z} = “Neste distrito, conheci um policial federal”

\begin{center}
\begin{table}[htbp]
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
 & \textbf{\begin{tabular}[c]{@{}c@{}}eu\\ moro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}moro\\ no\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}no\\ distrito\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}distrito\\ federal\end{tabular}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}neste\\ distrito\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}distrito\\ conheci\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}conheci\\ um\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}um\\ policial\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}policial\\ federal\end{tabular}}} \\ \midrule
\multicolumn{1}{l}{\textit{\textbf{u}}} & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
\multicolumn{1}{l}{\textit{\textbf{z}}} & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular}
\caption{Exemplo 2 de representação do modelo \textit{n-grams}}
\label{ngrams2}
\end{table}
\end{center}

\subsection{TF-IDF}

Em algumas situações, apenas avaliar a frequência de termos pode não ser o ideal. A forma com que a língua é estruturada favorece a dominância de uma série de termos dos quais não carregam em si conteúdo significativo para a realização de classificações. Estes, que são bastante recorrentes, não são bons indicadores de contexto por aparecerem em muitas frases independentes de suas classes.

A fim de lidar com o grau desbalanceado da relevância desses termos, o modelo TF-IDF\footnote{\textit{Term Frequency - Inverse Document Frequency}} propõe um fator de ponderação ($idf$) para que aqueles que mais aparecem nos documentos tenham uma representatividade menor \cite{Matsubara_Martins_Monard_2003}.

O cálculo do TF-IDF, definido pela Equação \ref{eq:tfidf}, descreve a frequência do termo $t_j$ em dado documento $d_i$ multiplicado pelo fator de ponderação, que varia entre 0 e o $\log{N}$, onde \textit{N} é o número de documentos do conjunto de dados e $d(t_j)$ é o número de documentos onde há a ocorrência do termo pelo menos uma vez.

\begin{equation}
\label{eq:tfidf}
tfidf(t_j, d_i) = freq(t_j, d_i)\times\log \frac{N}{d(t_j)}
\end{equation}

Ao aplicar a fórmula, o valor do fator de ponderação é igual a 0 quando o termo aparece em todos os documentos, mas quando aparece em apenas um, ele é $\log{N}$. Note também que, o valor do $tfidf$ é favorece a frequência do termo $t_j$ no documento $d_i$ e desfavorece a sua frequência na coleção $d(t_j)$. Em outras palavras, as particularidades de cada documento são acentuadas.

É possível também abordar representações alternativas deste modelo. Uma proposta apresentada por \citeauthor{Matsubara_Martins_Monard_2003}, denominada \textit{tflinear}, onde, basicamente, o fator de ponderação logarítmico é substituído por um fator linear que varia entre 0 e 1. Observe na Equação \ref{eq:tflinear}

\begin{equation}
\label{eq:tflinear}
tflinear(t_j, d_i) = freq(t_j, d_i)\times \left(1 - \frac{d(t_j)}{N}\right)
\end{equation}

No atual projeto, o TF-IDF foi utilizado em conjunto com o \textit{n-grams}, ou seja, os termos podem ser compostos por mais de uma palavra. Assim como os hiperparâmetros dos modelos de classificação, o valor de \textit{n} é definido durante a fase experimental, podendo, inclusive assumir vários valores dentro de um intervalo.

\subsection{Limpeza do Texto}
\label{limpeza}

Parte do pré-processamento do texto consiste na eliminação de informações desnecessárias para a classificação. Para tal, foram aplicadas algumas técnicas como remoção de acentuação, os caracteres maiúsculos foram transformados em minúsculos, além das detalhadas a seguir:

\begin{itemize}
\item \textbf{\textit{Stemização}}: Do inglês, \textit{stemming}, refere-se ao processo de reduzir as palavras à uma forma primitiva, como um radical. Este processo visa garantir que variações de uma mesma palavra sejam interpretadas pelo computador como uma palavra só. Dessa forma, o conjunto de palavras do vocabulário é reduzido, o que resulta numa economia de recursos. Outra vantagem é a generalização de algumas variações das palavras, como em gênero e número, identificando que elas carregam em si o mesmo sentido. Os termos “\textit{historiador}” e “\textit{historiadoras}” seriam reduzidas a um mesmo radical “\textit{histori}”, por exemplo.

\item \textbf{Remoção das \textit{stop words}}: As \textit{stop words} (palavras vazias) são palavras que agregam pouco ou nenhum valor semântico que deva ser levado em consideração pelo classificador. Geralmente, são as palavras mais comuns da língua, incluindo artigos, preposições, verbos de ligação, entre outras. Não existe um conjunto bem definido de quais palavras devem ser classificadas como palavras vazias, e, naturalmente, esse conjunto depende do idioma em questão. Em alguns casos, termos recorrentes do contexto, como jargões e gírias, também podem ser inclusos. Para o atual projeto, foi utilizado o conjunto de \textit{stopwords} em português do \textbf{NLTK}.
\end{itemize}

\section{Aprendizado de máquina}

Aprendizado de máquina (AM) pode ser definido como uma forma de Inteligência Artificial (IA) que permite que um sistema aprenda através de dados ao invés de uma programação explícita \cite{mlfordummyes:2018}. No início dos anos 90, a abordagem do AM para a classificação textual começou a ganhar popularidade e, eventualmente, tornou-se a técnica dominante para tal, pelo menos para a comunidade de pesquisadores \cite{Sebastiani_2002}.

Os modelos de Aprendizado de Máquina costumam ser divididos em 2 tipos: Supervisionado, do qual um conjunto de observações previamente rotuladas serve de insumo, o modelo tem como objetivo definir rótulos para novas observações, e o Não-Supervisionado, onde o modelo busca encontrar padrões ou estruturas intrínsecas nos dados de treino \cite{mwmachinelearning}. Observe a Figura \ref{fig:ml_types}.

\begin{figure}[!htb]
    \center{\includegraphics[width=\textwidth]
    {figuras/ml_techniques.png}}
    \caption{Diagrama sobre tipos de aprendizado de máquina}
    \label{fig:ml_types}
\end{figure}

A escolha de qual tipo de AM deve ser utilizado depende da natureza do problema que está sendo abordado e dos dados disponíveis. No atual trabalho, serão utilizados modelos de classificação, que são do tipo de Aprendizado Supervisionado. Estes modelos estão detalhados na seção \ref{Classificação}.


\section{Classificação}
\label{Classificação}

A classificação textual é a tarefa de atribuir um valor \textit{booleano} para cada par de observação e classe $(d_i, c_i)$. O valor é dado como \textbf{verdadeiro} se a observação $d_i$ for atribuída à classe $c_i$, caso contrário, \textbf{falso}. Existem diferentes tipos de classificações, que variam de acordo com a situação. Por exemplo, quando uma observação pode ser atribuída à apenas uma classe, ela é apelidada de \textit{sigle-label}, caso contrário, \textit{multi-label}. Um tipo caso especial de \textit{sigle-label} é a classificação binária, quando a observação deve, obrigatoriamente, ser atribuída à uma classe ou à sua classe complementar \cite{Sebastiani_2002}.

O atual trabalho contempla uma classificação binária, sendo as classes \textbf{C0}, que representa um \textbf{discurso normal}, e \textbf{C1}, representando \textbf{discurso de ódio ou linguagem inapropriada}.

Independente do modelo de AM, é necessário utilizar um conjunto de dados para que as classificações sejam realizadas. Os dados de treino são traduzidos em propriedades qualitativas ou quantitativas, as \textit{features}. Múltiplas \textit{features} são combinadas em um vetor e utilizadas como informação de entrada para o algoritmo com a sua classe correspondente \cite{Lundborg_2017}. Neste projeto, diversos de modelos de classificação são utilizados. Nas subseções deste capítulo, a descrição de cada um deles.

\subsection{Support Vector Machines}

O termo Support Vector Machines engloba um conjunto de modelos que buscam classificar os dados em 2 classes diferentes. Para tal, as features são analisadas e inseridas em um plano com N-1 features de dimensão, de modo que se forme uma “linha” que divide as 2 classes na maior margem possível entre elas. Essa linha é formada com o auxílio dos dois pontos mais próximos de cada classe. Através delas, duas linhas de suporte são traçadas, e a divisão principal é formada. Daí vem o nome “support vector”.

\begin{figure}[!htb]
    \center{\includegraphics[scale=0.3]
    {figuras/svm_example.png}}
    \caption{\label{fig:svm_example} Diagrama SVM para 2 features}
\end{figure}

O primeiro modelo que utiliza de vetores de suporte é o modelo Gradiente Descendente Estocástico, ou SDG (Stochastic Gradient Descent). Diferente de alguns modelos de vetores de suporte que, para realizar uma atualização de parâmetro necessita computar todas as features, o SDG faz uma iteração mais leve, de modo que apenas um ou um grupo de features é verificado a cada atualização. Desse modo, é possível trabalhar com um número alto de features e/ou exemplos de treino \cite{scikit-learn}.

O outro modelo baseado nesse princípio, é o Vetor de Suporte-C, ou SVC, implementado pela libsvm \cite{libsvm}. Esse modelo segue a ideia padrão de computar todas as features a cada atualização. Seu diferencial está num parâmetro, chamado parâmetro C. Quanto maior o valor desse parâmetro, menores as margens da linha de divisão serão, de modo que todos os pontos sejam classificados corretamente. Caso o valor seja pequeno, a linha de divisão busca por margens maiores, mesmo que algumas amostras sejam classificadas incorretamente.

Em nosso modelo de treinamento, o SVC foi utilizado em conjunto do TF-IDF, de modo que se defina várias features para o funcionamento correto do algoritmo. Contudo, o melhor valor para a variável C foi de 4.0, considerado bem alto. Por causa desse valor, é possível presumir que a linha que classifica as frases fica bem próxima do limite de ambas, uma vez que uma margem grande, alcançada através de um C pequena, não é desejado para a maior performance.

\begin{figure}[!htb]
    \center{\includegraphics[scale=0.3]
    {figuras/svm_example2.png}}
    \caption{\label{fig:svm_example2} Diagrama representando as mudanças da variável C}
\end{figure}

\subsection{Decision Tree/Random Forest}

Árvores de decisão é um modelo usado tanto para regressão, quanto para classificação \cite{decision-tree}. São fáceis de serem interpretadas, justamente por se tratar de um modelo baseado em uma estrutura de dados em que estamos acostumados a lidar. Além disso, o modelo suporta classificação numérica ou categórica, e consegue obter um desempenho por qualidade muito bom com relação a outros modelos de classificação, independente do tamanho do dataset \cite{performance-comparison}. Contudo, por se tratar de um algoritmo guloso, alguns problemas podem ser vistos nos resultados, como o sobreajuste, ou “overfitting”. O overfitting define um modelo estático que se adequa muito bem a um conjunto de dados já conhecido, mas que não consegue prever novos resultados caso um conjunto de dados diferente seja apresentado. Esse caso piora mais e mais, proporcionalmente igual à profundidade da árvore.  Com o objetivo de resolver isso, criou-se o modelo de Floresta randômica. O “Random forest” consiste em um combinado de árvores de decisão, onde o resultado de cada uma é unido, com o objetivo de encontrar uma predição mais estável e preciso.

Em nosso modelo de treinamento do algoritmo de Random forest, utilizamos um ‘n-gram’ de 1 a 4 palavras, de modo que as árvores possam crescer, dada a quantidade de features fornecida. A profundidade das árvores não foi limitada, mas foi definido um total de 100 árvores. Desse modo, mesmo que as árvores alcancem uma alta profundidade, o “overfitting” dos resultados não ocorre.

\subsection{Naive Bayes/Multinomial NB}
O modelo de Naive Bayes é baseado no teorema probabilístico de Bayes, e tem como forte premissa de que as “features”, ou palavras de cada sentença, são fortemente independentes. Utilizando do teorema, uma tabela de probabilidades é criada, utilizando a frequência de ocorrência das palavras. Para criação de tal tabela, o TF-IDF é uma das opções para a extração dessas features. A partir dessa tabela, o classificador de Naive Bayes analisa as palavras, de acordo com a probabilidade da feature ser “positiva” ou “negativa”, caso seja utilizado para se analisar o sentimento de uma frase, por exemplo.

O Multinomial Naive Bayes é uma extensão do modelo de Naive Bayes, mas com a utilização da distribuição multinomial para cada feature. Desse modo, o modelo Multinomial NB funciona bem para dados que podem ser contados, se tornando um bom modelo para classificação de tópicos, por exemplo \cite{russel-ia}.

Em nosso modelo de treinamento, tanto o modelo tradicional quanto o multinomial funcionam melhor com o dataset isento de stop words, pois a alta frequência das mesmas pode atrapalhar a eficiência do algoritmo. O n-gram que gerou o melhor resultado foi com apenas uma palavra. Isso pode ser explicado pelo fato de que frases que contém discurso de ódio são caracterizadas por palavras específicas, raramente utilizadas.

\subsection{MLP}

O MLP, ou Perceptron de múltiplas camadas, é um modelo de treinamento que tem como base a simulação do raciocínio humano. Seu funcionamento mais básico consiste em ao menos 3 camadas: A camada de entrada (onde as features serão passadas), a camada oculta (onde os cálculos são realizados), e a camada de saída. As camadas são compostas por neurônios, que realizam o cálculo utilizando de informações passadas pelos neurônios das camadas anteriores. Num MLP, podem haver várias camadas ocultas, dependendo do tipo de problema a ser resolvido. Cada camada oculta busca transformar o valor da camada anterior, utilizando da soma linear dos pesos e de uma função de ativação não linear, como a função tangente hiperbólica. Ao chegar na última camada oculta, a camada de saída recebe seu valor e o transforma em valores de saída.

Em nosso modelo de treino, foi utilizado apenas uma camada oculta, contendo 100 neurônios. Este valor busca cobrir casos em que haja muitas features (frases muito grandes podem resultar em altos valores de features), de modo que todas sejam levadas em consideração. Na maioria dos casos, menos de 20 neurônios seriam necessários para classificar a frase corretamente. Além disso, como a escolha é binária, apenas uma camada oculta é necessária, de modo que se crie uma “linha” que classifica a frase.

\begin{figure}[!htb]
    \center{\includegraphics[scale=0.3]
    {figuras/mlp_example.png}}
    \caption{\label{fig:MLP} Diagrama MLP,  4 features de entrada, 1 camada oculta, 8 neurônios}
\end{figure}